{
  "id": "250228064042483872",
  "name": "ieee-transformer",
  "model_path": "ieee-transformer_250228064042483872.pt",
  "batch": 8,
  "epochs": 31,
  "lr": 0.001,
  "enable_fp16": true,
  "grad_step": 4,
  "warmup_steps": 30,
  "lr_decay_factor": 0.5,
  "weight_decay": 0.001,
  "patience": 30,
  "tag": "IEEE_23",
  "train": "ieee_train.pt",
  "test": "ieee_test.pt",
  "val": "ieee_val.pt",
  "channels": 19,
  "length": 9250,
  "num_classes": 2,
  "embed_dim": 64,
  "num_heads": 4,
  "num_blocks": 4,
  "block_hidden_dim": 64,
  "fc_hidden_dim": 32,
  "dropout": 0.1,
  "accuracy": 0.9722222222222222,
  "f1-score": 0.975609756097561,
  "recall": 0.9523809523809523,
  "auc": 0.9761904761904762,
  "_comment": "Using FP16 precision accelerated training significantly without sacrificing performance. Increasing the gradient step size further improved results(2->4). However, the validation loss plateaued, suggesting the model is overfitting."
}