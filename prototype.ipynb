{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "Train Base ViT model for IEEE EEG dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import secrets\n",
    "import gc\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm, trange\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchsummary import summary\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def join_path(*args):\n",
    "    return os.path.join(\"/content/drive/MyDrive\", *args)\n",
    "\n",
    "def clear():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create unique ID\n",
    "while True:\n",
    "    experiment_id = secrets.token_hex(8)\n",
    "    if not os.path.exists(join_path(f\"{experiment_id}.pth\")) and not os.path.exists(join_path(f\"{experiment_id}.json\")):\n",
    "        break\n",
    "print(\"ID:\", experiment_id)\n",
    "\n",
    "# Fix random seed\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Settings\n",
    "ARGS = {\n",
    "    \"id\": experiment_id,\n",
    "    \"name\": \"4 Head Self attention\",\n",
    "    \"model_path\": join_path(f\"{experiment_id}.pth\"),\n",
    "    \"batch\": 256,\n",
    "    \"grad_step\": 1,\n",
    "    \"epochs\": 100,\n",
    "    \"lr\": 1e-4,\n",
    "    \"warmup_steps\": 10,\n",
    "    \"weight_decay\": 1e-2,\n",
    "    \"patience\": 2,\n",
    "}\n",
    "DATA = {\n",
    "    \"train_path\": join_path(\"data\", \"train.pt\"),\n",
    "    \"test_path\": join_path(\"data\", \"test.pt\"),\n",
    "    \"val_path\": join_path(\"data\", \"val.pt\"),\n",
    "    \"channel\": 19,\n",
    "    \"length\": 2560,\n",
    "    \"labels\": [\"control\", \"ADHD\"],\n",
    "}\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class EarlyStopping(object):\n",
    "    \"\"\"Stop training when loss does not decrease\n",
    "\n",
    "    :param patience: number of epochs to wait before stopping\n",
    "    :param save_path: path to save the best model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, patience, save_path):\n",
    "        self._min_loss = np.inf\n",
    "        self._patience = patience\n",
    "        self._path = save_path\n",
    "        self.__counter = 0\n",
    "\n",
    "    def should_stop(self, model, loss):\n",
    "        \"\"\"Check if training should stop\n",
    "\n",
    "        :param model: model to save\n",
    "        :param loss: current loss\n",
    "        \"\"\"\n",
    "        if loss < self._min_loss:\n",
    "            self._min_loss = loss\n",
    "            self.__counter = 0\n",
    "            torch.save(model.state_dict(), self._path)\n",
    "        elif loss > self._min_loss:\n",
    "            self.__counter += 1\n",
    "            if self.__counter >= self._patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def load(self, model):\n",
    "        \"\"\"Load best model\n",
    "\n",
    "        :param model: model structure\n",
    "        \"\"\"\n",
    "        model.load_state_dict(torch.load(self._path))\n",
    "        return model\n",
    "\n",
    "    @property\n",
    "    def patience(self):\n",
    "        \"\"\"Return patience\n",
    "\n",
    "        To calculate the check point:\n",
    "        >>> stopper = EarlyStopping(...)\n",
    "        >>> train()\n",
    "        >>> check_point = epoch - stopper.patience\n",
    "        \"\"\"\n",
    "        return self._patience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class WarmupScheduler:\n",
    "    \"\"\"Warmup learning rate and dynamically adjusts learning rate based on training loss.\n",
    "\n",
    "    :param optimizer: torch optimizer\n",
    "    :param initial_lr: initial learning rate\n",
    "    :param min_lr: minimum learning rate\n",
    "    :param warmup_steps: number of warmup steps\n",
    "    :param decay_factor: decay factor\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, optimizer, initial_lr, min_lr=1e-6, warmup_steps=10, decay_factor=10\n",
    "    ):\n",
    "        self.optimizer = optimizer\n",
    "        self.initial_lr = initial_lr\n",
    "        self.min_lr = min_lr\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.decay_factor = decay_factor\n",
    "\n",
    "        assert self.warmup_steps > 0, \"Warmup steps must be greater than 0\"\n",
    "        assert self.decay_factor > 1, \"Decay factor must be greater than 1\"\n",
    "\n",
    "        self.global_step = 0\n",
    "        self.best_loss = float(\"inf\")\n",
    "\n",
    "        # Store initial learning rates\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group[\"lr\"] = 0\n",
    "\n",
    "    def step(self, loss):\n",
    "        \"\"\"Update learning rate based on current loss.\"\"\"\n",
    "        self.global_step += 1\n",
    "\n",
    "        if self.global_step <= self.warmup_steps:\n",
    "            # Linear warmup\n",
    "            warmup_lr = self.initial_lr * (self.global_step / self.warmup_steps)\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                param_group[\"lr\"] = warmup_lr\n",
    "        else:\n",
    "            # Check if loss increased\n",
    "            if loss > self.best_loss:\n",
    "                for param_group in self.optimizer.param_groups:\n",
    "                    new_lr = max(param_group[\"lr\"] / self.decay_factor, self.min_lr)\n",
    "                    param_group[\"lr\"] = new_lr\n",
    "            self.best_loss = min(self.best_loss, loss)\n",
    "\n",
    "    def get_lr(self):\n",
    "        \"\"\"Return current learning rates.\"\"\"\n",
    "        return [param_group[\"lr\"] for param_group in self.optimizer.param_groups]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        self.data = torch.load(file_path, mmap=True) # lazy load\n",
    "        self.eeg = self.data[\"data\"]\n",
    "        self.labels = self.data[\"label\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.eeg[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = EEGDataset(DATA[\"train_path\"])\n",
    "val_dataset = EEGDataset(DATA[\"val_path\"])\n",
    "test_dataset = EEGDataset(DATA[\"test_path\"])\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=ARGS[\"batch\"], shuffle=True\n",
    ")\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=ARGS[\"batch\"])\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=ARGS[\"batch\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_dim):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.self_attention = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_dim, embed_dim),\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        # Multi-head Attention\n",
    "        x = self.norm1(input)\n",
    "        x, _ = self.self_attention(x, x, x, need_weights=False)\n",
    "\n",
    "        # Add & Norm\n",
    "        x_ = self.norm1(x + input)\n",
    "\n",
    "        # Feed Forward\n",
    "        x_ = self.mlp(x_)\n",
    "\n",
    "        # Add & Norm\n",
    "        x = self.norm2(x + x_)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class EEGTransformer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, num_block, num_class, seq_length, mlp_dim, fc_dim):\n",
    "        super(EEGTransformer, self).__init__()\n",
    "\n",
    "        # Embedding\n",
    "        self.embedding = nn.Conv1d(19, embed_dim, kernel_size=3, padding=1)\n",
    "        self.pos_embedding = nn.Parameter(torch.empty(1, seq_length, embed_dim).normal_(std=0.02))\n",
    "\n",
    "        # Attention Blocks\n",
    "        attention_blocks: OrderedDict[str, nn.Module] = OrderedDict()\n",
    "        for i in range(num_block):\n",
    "            attention_blocks[f\"attention_block_{i}\"] = AttentionBlock(embed_dim, num_heads, mlp_dim)\n",
    "        self.encoder = nn.Sequential(attention_blocks)\n",
    "\n",
    "        # Decoding layers\n",
    "        self.global_max_pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(1, -1),\n",
    "            nn.Linear(embed_dim, fc_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(fc_dim, num_class),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.embedding(input)\n",
    "        x = x + self.pos_embedding\n",
    "        x = self.encoder(x)\n",
    "        x = self.global_max_pool(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model = EEGTransformer().to(DEVICE)\n",
    "summary(model, (1, DATA[\"length\"], DATA[\"channel\"]), device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss().to(DEVICE)\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=ARGS[\"lr\"],\n",
    "    weight_decay=ARGS[\"weight_decay\"],\n",
    ")\n",
    "scheduler = WarmupScheduler(optimizer, ARGS[\"lr\"], warmup_steps=ARGS[\"warmup_steps\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, criterion, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for signal, label in val_loader:\n",
    "            signal = signal.to(device)\n",
    "            label = label.to(device)\n",
    "            output = model(signal)\n",
    "\n",
    "            batch_loss = criterion(output.logits, label.long())\n",
    "            val_loss += batch_loss.item()\n",
    "\n",
    "        return val_loss / len(val_loader)\n",
    "\n",
    "\n",
    "def train(model, optimizer, scheduler, criterion, train_loader, val_loader, device):\n",
    "    clear()\n",
    "\n",
    "    model_path = ARGS[\"model_path\"]\n",
    "    grad_step = ARGS[\"grad_step\"]\n",
    "    epoch_trange = trange(1, ARGS[\"epochs\"] + 1)\n",
    "    early_stopper = EarlyStopping(ARGS[\"patience\"], model_path)\n",
    "\n",
    "    model.zero_grad()\n",
    "\n",
    "    for epoch in epoch_trange:\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_id, (signal, label) in enumerate(train_loader, start=1):\n",
    "\n",
    "            output = model(signal)\n",
    "\n",
    "            batch_loss = criterion(output.logits, label.long())\n",
    "            train_loss += batch_loss.item()\n",
    "\n",
    "            batch_loss /= grad_step\n",
    "            batch_loss.backward()\n",
    "\n",
    "            # Batch Accumulation\n",
    "            if batch_id % grad_step == 0:\n",
    "                optimizer.step()\n",
    "                model.zero_grad()\n",
    "\n",
    "        # Validate Training Epoch\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss = evaluate(model, criterion, val_loader, device)\n",
    "        tqdm.write(\n",
    "            f\"Epoch {epoch}, Train-Loss: {train_loss:.5f},  Val-Loss: {val_loss:.5f}\"\n",
    "        )\n",
    "\n",
    "        # Early stopping\n",
    "        if early_stopper.should_stop(model, val_loss):\n",
    "            break\n",
    "\n",
    "        # Learning Rate Scheduling\n",
    "        scheduler.step(train_loss)\n",
    "\n",
    "    tqdm.write(f\"\\n--Check point: [Epoch: {epoch - early_stopper.patience}]\")\n",
    "    model = early_stopper.load(model)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model = train(model, optimizer, scheduler, loss_fn, train_dataloader, val_dataloader, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def test(model, test_loader, device):\n",
    "    model.eval()\n",
    "    y_pred = list()\n",
    "    y_true = list()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for signal, label in test_loader:\n",
    "            signal = signal.to(device)\n",
    "            label = label.to(device)\n",
    "            output = model(signal)\n",
    "\n",
    "            y_pred.extend(output.logits.argmax(1).detach().cpu().numpy())\n",
    "            y_true.extend(label.detach().cpu().numpy())\n",
    "\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "        auc_value = auc(fpr, tpr)\n",
    "\n",
    "        return {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"f1-score\": f1,\n",
    "            \"auc\": auc_value,\n",
    "            \"roc-curve\": (fpr, tpr),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "metrics = test(model, test_dataloader, DEVICE)\n",
    "print(\"Accuracy:\", metrics[\"accuracy\"])\n",
    "print(\"F1-Score:\", metrics[\"f1-score\"])\n",
    "print(\"AUC:\", metrics[\"auc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(*metrics[\"roc-curve\"], color='blue')\n",
    "plt.plot([0, 1], [0, 1], color='grey', linestyle='--')  # Baseline\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "experiment_result = {\n",
    "    \"id\": ARGS[\"id\"],\n",
    "    \"weights\": ARGS[\"model_path\"],\n",
    "    \"batch\": (ARGS[\"batch\"], ARGS[\"grad_step\"]), # Batch size with Gradient Accumulation\n",
    "    \"lr\": ARGS[\"lr\"],\n",
    "    \"weight_decay\": ARGS[\"weight_decay\"],\n",
    "    \"accuracy\": metrics[\"accuracy\"],\n",
    "    \"f1-score\": metrics[\"f1-score\"],\n",
    "    \"auc\": metrics[\"auc\"],\n",
    "}\n",
    "\n",
    "for key, value in experiment_result.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Save result in json format\n",
    "with open(join_path(f\"{ARGS['id']}.json\"), \"w\") as f:\n",
    "    json.dump(experiment_result, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
